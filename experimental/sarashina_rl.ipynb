{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b061e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e04fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 0) データ（最小例）\n",
    "#   実運用ではここを自前の preference dataset に置き換えます\n",
    "#   形式: prompt / chosen / rejected\n",
    "# -----------------------------\n",
    "data = [\n",
    "    {\n",
    "        \"prompt\": \"<|system|>ユーザの発言を平易化し，さらに書き言葉から話し言葉に変換してください。</s><|user|>千葉工業大学は千葉県習志野市にある工業大学である。</s>\",\n",
    "        \"chosen\": \"千葉工大は、千葉の習志野にある工業系の大学なんだ。\",\n",
    "        \"rejected\": \"千葉工業大学は、千葉の習志野にある工業系の大学だよ。簡単に言うと、工学を学ぶための大学なんだ。場所は、千葉県の習志野ってところにあって、分かりやすい場所にあるよ\",\n",
    "    },\n",
    "]\n",
    "train_ds = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) モデルとトークナイザ\n",
    "# -----------------------------\n",
    "# model_name = \"gpt2\"  # デモ用。実際は任意の CausalLM に置換\n",
    "model_name = \"sbintuitions/sarashina2.2-3b-instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # DPO では padding が必要になることが多い\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "\n",
    "# reference model（π_ref）：固定。policy と同一初期値から開始するのが典型\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=base_model.dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ffe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) PEFT (LoRA) を policy 側だけに適用\n",
    "#   - ref_model は固定のまま（LoRA すら載せないのが「最小」）\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                     # 8 or 16 がまずは無難\n",
    "    lora_alpha=16,           # 通常 r の 2 倍\n",
    "    lora_dropout=0.05,       # DPO では 0.0〜0.1\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) DPO 設定（β、バッチ等）\n",
    "#   - beta が 2.5 の β\n",
    "# -----------------------------\n",
    "dpo_args = DPOConfig(\n",
    "    output_dir=\"./dpo_lora_out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    beta=0.1,\n",
    "    logging_steps=1,\n",
    "    save_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    # 安定性のために max_length / max_prompt_length を入れるのが一般的\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e815c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Trainer\n",
    "#   - 内部で log πθ(chosen|prompt), log πθ(rejected|prompt)\n",
    "#          log πref(chosen|prompt), log πref(rejected|prompt)\n",
    "#     を計算して 2.5 の損失を作ります\n",
    "# -----------------------------\n",
    "trainer = DPOTrainer(\n",
    "    model=model,                 # πθ（LoRA付き）\n",
    "    ref_model=ref_model,         # π_ref（固定）\n",
    "    args=dpo_args,\n",
    "    train_dataset=train_ds,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
